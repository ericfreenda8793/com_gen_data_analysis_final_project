{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "genotypes = pd.read_csv('tomatoes/Genotypic_data_maf10_min10_291acc.txt', index_col=0)\n",
    "phenotype = pd.read_csv('tomatoes/phenodata_BLUP_2012.txt', sep='\\t', index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_maf(df):\n",
    "    # Calculate minor allele frequency\n",
    "    maf = df.apply(lambda x: min(x.mean(), 1-x.mean()), axis=0)\n",
    "    return maf\n",
    "\n",
    "def ld_pruning(df, threshold=0.5):\n",
    "    # Calculate correlation matrix\n",
    "    corr = df.corr()\n",
    "    # Identify pairs of SNPs with correlation greater than the threshold\n",
    "    # Avoid double removal and self-comparison (i.e., diagonal elements)\n",
    "    to_remove = set()\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] > threshold:\n",
    "                to_remove.add(corr.columns[j])\n",
    "    return df.drop(columns=to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MAF filtering\n",
    "maf = calculate_maf(genotypes)\n",
    "maf_threshold = 0.01  # Set MAF threshold\n",
    "genotypes_filtered = genotypes.loc[:, maf >= maf_threshold]\n",
    "\n",
    "# Apply LD pruning\n",
    "genotypes_pruned = ld_pruning(genotypes_filtered, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'genotypes' and 'phenotype' are already loaded and aligned by their indices\n",
    "# now we check for missing data\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "genotypes_imputed = pd.DataFrame(imputer.fit_transform(genotypes_pruned), columns=genotypes_pruned.columns)\n",
    "phenotype_imputed = pd.DataFrame(imputer.fit_transform(phenotype), columns=phenotype.columns)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "genotypes_scaled = pd.DataFrame(scaler.fit_transform(genotypes_imputed), columns=genotypes_pruned.columns)\n",
    "\n",
    "# Check for any remaining NaNs or infinities\n",
    "genotypes_scaled = genotypes_scaled.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "phenotype_scaled = pd.DataFrame(scaler.fit_transform(phenotype_imputed), columns=phenotype.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 291\n",
      "Number of features: 9381\n"
     ]
    }
   ],
   "source": [
    "# Extract the AVGROW97 column from the phenotype dataframe\n",
    "y = phenotype_scaled['AVGROW97']\n",
    "# Construct X from the genotype dataframe\n",
    "X = genotypes_scaled\n",
    "\n",
    "# Determine the number of samples (rows) and features (columns)\n",
    "num_samples, num_features = X.shape\n",
    "\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=5)\n",
    "principal_components = pca.fit_transform(genotypes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance by PCA components\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Feature Selection with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Lasso with built-in cross-validation to choose the best alpha\n",
    "lasso = LassoCV(cv=5, random_state=0, max_iter=10000).fit(X, y)\n",
    "selected_features_lasso = np.where(lasso.coef_ != 0)[0]\n",
    "# save selected features from Lasso\n",
    "np.savetxt(\"selected_features_lasso.txt\", selected_features_lasso, fmt='%d')\n",
    "# save Lasso coefficients\n",
    "np.savetxt(\"lasso_coefficients.txt\", lasso.coef_, fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

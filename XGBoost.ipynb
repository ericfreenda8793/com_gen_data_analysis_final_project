{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "genotypes = pd.read_csv('tomatoes/Genotypic_data_maf10_min10_291acc.txt', index_col=0)\n",
    "phenotype = pd.read_csv('tomatoes/phenodata_BLUP_2012.txt', sep='\\t', index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_maf(df):\n",
    "    # Calculate minor allele frequency\n",
    "    maf = df.apply(lambda x: min(x.mean(), 1-x.mean()), axis=0)\n",
    "    return maf\n",
    "\n",
    "def ld_pruning(df, threshold=0.5):\n",
    "    # Calculate correlation matrix\n",
    "    corr = df.corr()\n",
    "    # Identify pairs of SNPs with correlation greater than the threshold\n",
    "    # Avoid double removal and self-comparison (i.e., diagonal elements)\n",
    "    to_remove = set()\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i+1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] > threshold:\n",
    "                to_remove.add(corr.columns[j])\n",
    "    return df.drop(columns=to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MAF filtering\n",
    "maf = calculate_maf(genotypes)\n",
    "maf_threshold = 0.01  # Set MAF threshold\n",
    "genotypes_filtered = genotypes.loc[:, maf >= maf_threshold]\n",
    "\n",
    "# Apply LD pruning\n",
    "genotypes_pruned = ld_pruning(genotypes_filtered, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'genotypes' and 'phenotype' are already loaded and aligned by their indices\n",
    "# now we check for missing data\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "genotypes_imputed = pd.DataFrame(imputer.fit_transform(genotypes_pruned), columns=genotypes_pruned.columns)\n",
    "phenotype_imputed = pd.DataFrame(imputer.fit_transform(phenotype), columns=phenotype.columns)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "genotypes_scaled = pd.DataFrame(scaler.fit_transform(genotypes_imputed), columns=genotypes_pruned.columns)\n",
    "\n",
    "# Check for any remaining NaNs or infinities\n",
    "genotypes_scaled = genotypes_scaled.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "phenotype_scaled = pd.DataFrame(scaler.fit_transform(phenotype_imputed), columns=phenotype.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the AVGROW97 column from the phenotype dataframe\n",
    "y = phenotype_scaled['AVGROW97']\n",
    "# Construct X from the genotype dataframe\n",
    "X = genotypes_scaled\n",
    "\n",
    "# Determine the number of samples (rows) and features (columns)\n",
    "num_samples, num_features = X.shape\n",
    "\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. XGBoost Feature Selection with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# XGBoost hyperparameters grid\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "}\n",
    "\n",
    "# Randomized search with cross-validation\n",
    "xgb_random_search = RandomizedSearchCV(xgb.XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss'), \n",
    "                                       xgb_params, n_iter=10, cv=5, random_state=0)\n",
    "xgb_random_search.fit(X, y)\n",
    "selected_features_xgb = np.argsort(xgb_random_search.best_estimator_.feature_importances_)[::-1]\n",
    "# Save selected features from XGBoost\n",
    "np.savetxt(\"selected_features_xgb.txt\", selected_features_xgb, fmt='%d')\n",
    "# XGBoost feature importances\n",
    "np.savetxt(\"xgboost_importances.txt\", xgb_random_search.best_estimator_.feature_importances_, fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
